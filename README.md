# AI-TestTask
Test Task for int20h hackathon 2026 in AI category

Автоматизована система на базі мультиагентної LLM-взаємодії для генерації реалістичних діалогів служби підтримки (із симуляцією помилок) та їх глибокого аналізу.

## Опис завдання та наше рішення

### Проблема:
Навчання та тестування сучасних ШІ-асистентів для служби підтримки потребує величезної кількості якісних даних. Використовувати реальні чати важко через конфіденційність (NDA) та приватні дані користувачів. З іншого боку, звичайні згенеровані датасети часто виглядають "пластиковими", ідеалізованими і не містять реалістичних крайових випадків (edge cases): агресії клієнтів, некомпетентності операторів або раптових змін теми. Крім того, ручна розмітка таких даних (тегування інтенту, оцінка якості) — це довго і дорого.

### Наше рішення:
Ми розробили комплексний Data-Pipeline, який не лише створює реалістичні дані, але й автоматично їх аналізує та валідує. Проєкт складається з трьох ключових рівнів:
#### 1. Генерація
Ми створили середовище, де спілкуються два незалежні ШІ-агенти:
 - **Client Agent:** Отримує одну з 5 унікальних персоналій (від "агресивного сеньйор-девелопера" до "розгубленого літнього користувача"). Він динамічно змінює свої емоції в процесі діалогу та може навіть перервати чат (rage-quit), якщо оператор працює погано.
 - **Support Agent:** Симулює роботу оператора, але з "родзинкою" — за допомогою ймовірнісних ваг він іноді навмисно робить помилки (видає нерелевантну інформацію, грубіянить або дає неправильні інструкції).

#### 2. Аналіз
Наш модуль Analyzer виступає в ролі незалежного QA-спеціаліста. Він "читає" сирі діалоги та за допомогою структурованого JSON-виводу (через Pydantic та Instructor) автоматично визначає:
 - Категорію звернення.
 - Фінальний рівень задоволеності клієнта.
 - Оцінку якості роботи оператора.
 - Конкретний список допущених оператором помилок.

#### 3. Тестування
Оскільки ШІ може галюцинувати, нам потрібен механізм контролю. Модуль Tester автоматично порівнює результати роботи Аналізатора з еталонними метаданими етапу генерації. Він виявляє розбіжності (наприклад, пропущені помилки оператора) та перевіряє пайплайн на детермінованість при temperature=0.0. Це гарантує, що наша система аналітики працює стабільно і передбачувано.

## Логіка роботи
Наша система працює як автоматизований конвеєр: від створення сирих даних до їх глибокого семантичного аналізу та тестування.

### 1. Модуль Generator (generate.py)
Відповідає за симуляцію живого спілкування.
 - **Взаємодія агентів:** ClientAgent отримує випадкову персону (наприклад, "агресивний сеньйор" або "розгублена бабуся") та стартову проблему. SupportAgent намагається йому допомогти, але за заданими ймовірностями може навмисно обрати погану стратегію (дати неправильну відповідь, розповісти нерелевантну історію тощо).
 - **Контекстне вікно:** Щоб обійти ліміти на кількість токенів (Error 413 Payload Too Large) у довгих розмовах, агенти використовують "ковзне вікно" - пам'ятають системні правила та лише останні кілька реплік.
 - **Результат:** Згенеровані діалоги зберігаються у output/dataset.json, а очікувані (еталонні) метрики — у output/validation.json.

### 2. Модуль Analyzer (analyze.py)
Виступає в ролі ШІ-аудитора, який перевіряє роботу служби підтримки.
 - **Аналіз чату:** Аналізатор отримує "голий" текст діалогу і за допомогою спеціального системного промпту робить висновки.
 - **Структурований вивід:** Замість звичайного тексту, LLM змушена повернути дані, що суворо відповідають Pydantic-схемі AnalysisResponse. Вона визначає: intent (тему), satisfaction (задоволеність), quality_score (оцінку 1-5) та agent_mistakes (список допущених помилок).

### 3. Модуль Tester (tester/)
 - Автоматично порівнює result.json (висновки Аналізатора) та validation.json (реальні дані від Генератора).
 - Виявляє пропущені помилки (missing expected) або вигадані ШІ-галюцинації (unexpected actual) та логує їх для подальшого покращення промптів.


## Технологічний стек
Ми обрали сучасні та надійні інструменти для роботи з LLM:
 - **Python 3.10+** — основна мова розробки.
 - **Docker & Docker Compose** — контейнеризація для легкого запуску. Використано volume-маунтинг, тому всі згенеровані файли миттєво з'являються на хост-машині.
 - **OpenAI SDK + Groq API** — для швидкої та дешевої генерації тексту. Використовуємо потужні opensource-моделі (Llama-3 70b) через сумісний з OpenAI інтерфейс.
 - **Instructor + Pydantic** — магія структурованих даних. Замість того, щоб парсити сирий текст і писати регулярні вирази, ми гарантуємо, що модель завжди повертає валідний JSON з потрібними полями (списками, числами, булевими значеннями).
 - **Tenacity** — бібліотека для розумних ретраїв. Налаштована на експоненційне очікування (Exponential Backoff), що дозволяє елегантно обходити жорсткі ліміти запитів (Rate Limits) від API провайдера без "падіння" скрипта.

## Структура проєкту
Код розділено на логічні модулі: бізнес-логіка відділена від промптів, що дозволяє легко масштабувати систему.
```Plaintext
AI-TestTask
├── data_temp/             # Зберігаються метадані для валідації та результати роботи тестувальника
├── output/                # Зберігаються згенеровані датасети, результати аналізу
├── logs/                  # Зберігаються логи
├── source/                # Основний вихідний код
│   ├── analyzer/          # Модуль аналітики
│   ├── generator/         # Модуль генерації 
│   ├── tester/            # QA-модуль для перевірки збігів
│   └── utils/             # Допоміжні скрипти 
├── docker-compose.yml     # Налаштування контейнерів
├── .env.example           # Приклад конфігурації
├── analyze.py             # Точка входу: Аналізатор
└── generate.py            # Точка входу: Генератор
```

# Запуск проєкту
1. **Налаштуйте змінні середовища:**
    Перейменуйте `.env.example` в `.env` та вставте власний ключ.
2. **Запустіть Docker-середовище:**
   Проєкт використовує volume-маунтинг, тому всі зміни в коді та згенеровані дані миттєво синхронізуються з вашою локальною файловою системою.
   При додаванні нових бібліотек також використовуйте цю команду.
```bash
docker-compose up -d --build
```

## Має 2 точки входу:
 - Generate (Генерує dataset: розмови клієнта та support-агента)
 - Analyze (Аналізує роботу support-агента, та видає результат в json)

### Generate:
```bash
docker-compose exec app python generate.py
```

### Analyze:

```bash
docker-compose exec app python analyze.py
```

# Результати роботи
Всі результати автоматично зберігаються та структуруються у папці output/ у корні проєкту:
 - output/dataset.json — Сирі згенеровані діалоги.
 - data_temp/validation.json — Еталонні метадані.
 - output/result.json — Висновки аналізатора.
 - data_temp/mismatch_X.json — Звіти тестера про знайдені розбіжності.